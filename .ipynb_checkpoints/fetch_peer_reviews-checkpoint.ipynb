{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keyring        # for loading api token\n",
    "import urllib.request # for encoding URL parameters\n",
    "import pandas as pd   # for handling data frames\n",
    "import json           # for handling json\n",
    "import os             # for outputting the absolute path of the file containing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Authentication\n",
    "\n",
    "Please read the Authentication section of the [README](README.ipynb) to set up your acess token for the following code.  <span style=\"color:red\"> WARNING: the below code will load the access token you saved from the README.ipynb</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = keyring.get_password(\"system\", \"canvas_token\");\n",
    "print(\"Loaded token with %d characters.\" % (len(token)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "The following configuration does not frequently change. Input your course ID found in the url of your canvas course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "course_id = int(input(\"Enter your course id here: \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next one can change based on your rubric.  This can be found from the data returned from the assignments notebook or by looking at the outcomes section of canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_id = int(input(\"Enter your rubric id here: \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "        'base_url': 'https://usfca.instructure.com',\n",
    "        'course_id': course_id,\n",
    "        'rubric': rubric_id\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Request Setup\n",
    "\n",
    "The following sets up the REST API request to get a rubric:\n",
    "\n",
    "<https://canvas.instructure.com/doc/api/rubrics.html#method.rubrics_api.show>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_format = '{base_url}/api/v1/courses/{course_id}/rubrics/{rubric}'\n",
    "api_url = api_format.format(**config)\n",
    "print(\"URL:\", api_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'per_page': 200,\n",
    "    'include[]': 'assessments',\n",
    "    'style':'full'\n",
    "}\n",
    "\n",
    "encoded = urllib.parse.urlencode(params)\n",
    "print(\"Params:\", encoded) # do not output api key "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest_call = '{}?access_token={}&{}'.format(api_url, token, encoded)\n",
    "print(\"REST call is %d characters.\" % (len(rest_call)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetch Data\n",
    "\n",
    "Fetch the JSON data from the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# due to the format of this json object, some preprocessing using  the json library needs to be done then converted to a dataframe\n",
    "with urllib.request.urlopen(rest_call) as url:\n",
    "    data = json.loads(url.read().decode())  \n",
    "data = pd.DataFrame.from_dict(data, orient='index')\n",
    "print('Loaded {} rows and {} columns.'.format(*data.shape))\n",
    "# The column should be the rubric \n",
    "# the rows are the information about the rubric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output columns (should be one due to requesting one rubric)\n",
    "print('Columns:', list(data.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head() # look at the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain Assessments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempting to obtain assessments information if possible in the rubric.  These assessments are the feeedbacks students gave each other in the submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    assessments = data.loc['assessments']\n",
    "except KeyError:\n",
    "    print(\"There are no assessments here please input a new assignment or check that your access token is accurate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted = pd.DataFrame.from_dict(assessments) # we only need the assessments from the data gathered from the api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Assessments Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating dataframe out of the assessments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded = converted.explode(\"assessments\") # converted is a single row containing a list of all assessments of a rubric\n",
    "# we want to convert that list to many rows per feedback\n",
    "expanded.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expanded.loc[0, \"assessments\"] # testing lokking at a single feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling\n",
    "\n",
    "Some of the columns could use some wrangling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrangled = expanded.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the dictionary that corresponds to the feedback into a row of a dataframe for all rows\n",
    "assessments = wrangled.assessments.apply(pd.Series) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assessments.head() # see how output has changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rubric association has more information in the form of a dictionary so do the same as above\n",
    "rubric_association = assessments.rubric_association.apply(pd.Series) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming columns from rubric association since some have the same names as columns from the assessments\n",
    "rubric_association.columns = [\"rubric_association_\" + item for item in rubric_association.columns.tolist()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_association.head() # make sure columns are renamed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.concat([assessments, rubric_association], axis=1) # combine the riginal assessments with the rubric associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.loc[:,~reviews.columns.duplicated()] # removing any duplicated columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = reviews.explode(\"data\") # the data field corresponds to each question in the feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as done above, the data is in the format of a dictionary and can have the same column names\n",
    "reviews_data = reviews.data.apply(pd.Series)\n",
    "reviews_data.columns = [\"reviews_data_\" + item for item in reviews_data.columns.tolist()] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_data.head() # make sure columns are renamed properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.concat([reviews_data, reviews], axis=1) # combine the data again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.head() # done combining data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.drop(columns=[\"rubric_association\", \"data\"], inplace=True) # since both columns were expanded we can safely drop them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do this if you already saved a peer review to a file and want to add another to it from another assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peer_reviews = pd.read_csv('peer_reviews.csv')\n",
    "# final = pd.concat([final, peer_reviews], axis=0, sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output Results\n",
    "\n",
    "Output the results to CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'peer_reviews.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv(path, header = True, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.abspath('peer_reviews.csv') # the absolute path of the result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
